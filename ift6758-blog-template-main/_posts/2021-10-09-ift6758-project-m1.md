---
layout: post
title: IFT6758 Project
---

(TODO: tiny Intro to NHL hockey) 

This page focuses on the discussions based on the problem statements mentioned in Milestone-1 of the Project for IFT 6758.

## 1. Warm-Up

There have been numerous attempts to define metrics that evaluate a goalies performance and one of the most used metric has been the Save Percentage(SV%). However, there are multiple factors that directly effect the performance, and hence should ideally be taken into account for this evaluation.

One of the problems with save percentage is that the quality of shots faced can vary a lot from goalie to goalie. For example, a goalie who plays for a weak defence may face harder shots than average. Adjusted measures of save percetage can help account for this. For example, even strengh save percentage can give a clearer picture of the goalie's performance by removing powerplay scoring, which is very volatile and dependant on the quality of the team's penalty kill. Other even strengh metrics allow us to go deeper into this type of analysis. One such metric is HDsc SV% (save percentage against high-danger scoring chances at even strengh).
Another problem is that SV% does not take worload into account. If two goalies play at the same level but play in a different number of games, the one who plays more has a bigger impact on his team's success. One metric that can help looking at a goalie's total contribution for a season is even strength xGA - even strengh GA. xGA is an estimate of expected GA based on shots faced by location against league average shooting% from those locations. For example, in the 2017-2018 season, Anti Raanta let in 74 even strengh goals and had an xGA of 84.9. So for that season, he let it 14.9 less even strengh goals than expected (see 'NHL advanced' section of https://www.hockey-reference.com/players/r/raantan01.html).

![Distribution of Top 20 Goalies wrt the SV%](/figures/save_percentage_plot.png "Distribution of Top 20 Goalies wrt the SV%")


## 2. Data Acquisition

This section is a short tutorial on downloading the play-by-play data from both the regular seasons and the playoffs, using the NHL API.

### API Endpoint

The primary endpoint we use to fetch this data is 

```markdown
---
https://statsapi.web.nhl.com/api/v1/game/[GAME_ID]/feed/live/
---
```
As we can see that the only argument the API inputs is the game_ID. This game_ID is a 10-digit identifier where the first 4 digits correspond to the season, next 2 digits indicate the type of the game where 02= regular season and 03=playoffs. The final 4 digits identify the specific game number. For playoff games, the 2nd digit of the specific number gives the round of the playoffs, the 3rd digit specifies the matchup, and the 4th digit specifies the game (out of 7).

### API Requests

We use python's requests library to fetch game data for a given Game ID.

```python
def get_file(game_id, folder_path):
    """
    Check if file is already downloaded.
        If yes, return the file path
        If not, check if it exists on the nhl api.
            If it does, save it locally and return the path
            If it doesn't, return None
    """
    
    file_path = folder_path + str(game_id) + '.json'
    if not os.path.isfile(file_path):
        data = requests.get(f'https://statsapi.web.nhl.com/api/v1/game/{game_id}/feed/live/')
        if (data.status_code == 404):
            return None
        with open(file_path, 'w') as f:
            json.dump(data.json(), f)
            
    return file_path
```
Notice, that we're sending the request only if a JSON for the corresponding game_ID has not already been fetched.

### Forming the Game IDs

For the regular season, the game ID has the following structue:

```python
game_id = int(f'{season}{game_type}{game_number.zfill(4)}')
```

While for the playoffs, the logic is

```python
game_id = int(f'{season}{game_type}{playoff_round.zfill(2)}{matchup}{game_number}')
# where the game_number is out of 7
```

#### Finally,
We then iterate over the game numbers to form game_IDs as required and fetch the live data using the API endpoint.
To enable analysis on this data and avoid redundant calls to the API, we dump the result into an individual JSON each time.
