---
layout: post
title: Milestone 2
---
<div style="text-align: justify">
This post focuses on discussions based on the problem statements mentioned in Milestone-2 of the Project for IFT 6758.
    </div>

# Question 2

### 2.1

![image](./figures/milestone2/q_2_1_goals_distance.png)
![image](./figures/milestone2/q_2_1_shots_distance.png)
<div style="text-align: center"><i>Histograms of shot counts with respect to their distance to the net(goals on top and no-goals at the bottom)</i></div><br>

<div style="text-align: justify">
    We have displayed histograms of the shot counts from the 2015/16 to the 2018/19 seasons with regards to their distance to the net. We can see that very little shots are taken from more than 70 feet away. Furthermore, we can see that the further away players are, the less likely their shots are to convert into goals. This can be explained by the presence of defenders blocking their vision/their shots, by the reduced precision of their shot due to the distance or by the increased time the goalie has to block the shot. In both histograms, we also see small local peaks at ~170 feet way, which is discussed further in question 2.2 and 2.3.
    </div>



![image](./figures/milestone2/q_2_1_goals_angle.png)
![image](./figures/milestone2/q_2_1_shots_angle.png)
<div style="text-align: center"><i>Histograms of shots with respect to the shot angle to the net (goals on top and no-goals at the bottom)</i></div><br>

<div style="text-align: justify">
    We have displayed histograms of the shot counts from the 2015/16 to the 2018/19 seasons with regards to the shot angle to the net. We can see that goals are most often scored straight to the net (~0째 angle to the net). However, for shots taken, there are 3 peaks:  at 0째 and at +/- 30째 (+/- 5) angle, on either side of the net. Many offensive plays come from the side of the rink where players try to cut to the middle in order to get a better shot, which can explain the high proportion of shots taken at those angles.
    </div>

![image](./figures/milestone2/q_2_1_2D.png)
<div style="text-align: center"><i> 2D scatter plot and histograms of shot counts with respect to their distance (x-axis) and their angle (y-axis)</i></div><br>

<div style="text-align: justify">
    Looking at the 2D histogram, we see that the further away we are from the net, the less shots are taken at a big angle. Yet, closer shots taken at almost a 90째 degree angle are more likely to be converted into goals, which can seem contradictory since the attacker has less options to shoot.
    </div>

### 2.2
![image](./figures/milestone2/q_2_2_distance.png)
![image](./figures/milestone2/q_2_2_angle.png)
<div style="text-align: center"><i>Graphs relating goal rate and shot distance and relating goal rate and shot angle</i></div><br>

<div style="text-align: justify">
    Note that the x axis of the plots above are binned by percentile rather than by raw measurement. We chose to do it this way in order to have a large sample size in each bucket. These two plots confirm our previous observations. Shots are converted to goals at a higher rate when taken closer to the net and/or taken at a small absolute angle with respect to the net. The relationship is much stronger in the case of distance. We can observe that some shots convert to goals when taken at the greatest distance possible: usually, when teams are behind in score towards the end of the game, they will sub out their goalie in order to have an additional player on the rink - this gives the opportunity to the other team to shot in the empty from their zone.
    </div>

## 2.3

![image](./figures/milestone2/q_2_3_empty_nets.png)
![image](./figures/milestone2/q_2_3_non_empty_nets.png)

<div style="text-align: justify">
    The two histograms show the proportion of goals scored on empty and non-empty nets. The clear difference that is observable is the distance at which empty nets goals are scored compared to non-empty nets goals. Most goals that are scored on non-empty goals are relatively close to the net - as discussed earlier, the presence of the goalie makes it harder to score from afar. However,  when the nets are empty, the offensive team has more options and can choose to shoot at the net from almost anywhere on the rink.
    <br>
     The peak at ~170 feet in the second histogram is suspicious. It seems unlikely that this many goals are scored from this far away, which led us to believe some events had been attributed the wrong coordinates (i.e. having their coordinates on the wrong side of the rink). After filtering our dataframe to try to locate such events, we found that the goal scored by Anders Lee in the dataframe snippet below was logged on the wrong side of the rink. We see that Anders lee is right behind the other team's net when he scores, wheras our dataframe has him right behind his own net.
    </div>

![image](./figures/milestone2/anders_lee.png)

The goal can be seen [here](https://www.youtube.com/watch?v=CbdSJJ_bYOQ) at the 5:24 timestamp

# Question 3

## 3.1

![image](./figures/milestone2/Q3_heat_map.png)
<div style="text-align: center"><i>Heatmap of predictions on the validation set from the logistic regression trained using only the distance feature</i></div><br>

<div style="text-align: justify">
    Is it clear from the heatmap generated that the logistic regression model didn't properly classify every datapoint. Even though the accuracy was of 90.6%, there aren't any shots that were classified as goals, resulting in a bad representation of the dataset. This can be explained by the high level of imbalance in our dataset - out of all the shots taken, only a small proportion of them get converted into goals. The validation set has a ratio of shots to goals of 10:1 - a good accuracy was obtained by having the model predict the majority class for every datapoint. This shows that accracy score alone is not sufficient and it is required to evaluate the performance of the model using alternative metrics such as precision, recall, f1 and AUC.
    </div>

## 3.2 and 3.3

![image](./figures/milestone2/Q3_ROC_curve.png)
<div style="text-align: center"><i>Receiver Operating Characteristic curves of the logistic regression model trained on three different combinations of features and a random baseline</i></div><br>

<div style="text-align: justify">
The Area Under the Curve (AUC) is the measure of the ability of a classifier to distinguish between classes. It is used as a summary of the ROC curve. The higher the AUC, the better the performance of the model at distinguishing between the positive and negative classes. When the AUC is between 0.5 and 1, there is a high chance that the classifier will be able to distinguish the positive class values from the  negative class values. In our case, the model trained with distance and angle combined yields better classfication probabilities compared to distance feature and angle feature separately implying that distance and angle feature are similar in vector space. An AUC equal to 0.5 implies that the classifier is not able to distinguish between positive and negative class points but predicts random class or constant class for all the data points which we can confirm with the AUC attained (0.505) by sampling from a uniform distribution.
   </div> <br>

![image](./figures/milestone2/Q3_Goal_Rate.png)
<div style="text-align: center"><i>Goal rate curves of the logistic regression model trained on three different combinations of features and a random baseline</i></div><br>

<div style="text-align: justify">
 The higher the shot probability model percentile (from predicted probabilities) in a bin, the greater the probability of an actual goal (ground truth)
being in that bin i.e., having higher goal rate. This hypothesis can be validated with the plots from features distance and angle combined.
These two features follow a very similar pattern and exhibit coherence which makes them significant for analysis. However, the angle feature alone exhibits a simliar trend compared to the random baseline, making it less significant since it does not add any value to the models performace.
    </div><br>

![image](./figures/milestone2/Q3_Cum_Goal.png)
<div style="text-align: center"><i>Cumulative proportion of goals curves of the logistic regression model trained on three different combinations of features and a random baseline</i></div><br>

<div style="text-align: justify">
Once again it can be observed that the distance and angle features combined are correlated well when comparing the cumulative goal rate with shot probability model percentile. The angle feature is following the trend of random baseline once again making it irrelevant when considered separately.
    </div><br>

![image](./figures/milestone2/Q3_Calibration_Curve.png)
<div style="text-align: center"><i>Reliability diagram of the logistic regression model trained on three different combinations of features and a random baseline</i></div><br>

<div style="text-align: justify">
    For a perfectly calibrated reliability curve, the mean probabilities in a bin should match with the proportion of positives in that bin.
Examining the calibration plots corresponding to our selected features, none of them show reliability characteristic. Given the class imbalance,
the angle feature is calibrated well compared to the distance feature or a combination of the distance and angle features. The model trained with angle and distance is highly diverging from ideal curve.
    </div>

<div style="text-align: justify">
It can be determined that distance feature on its own is very much significant for the model's performance. The model's performance is slightly better when combined with the angle feature. Yet, the angle feature itself is not very deterministic and follows the trend of the random baseline.
    </div>

## 3.4

For the comet experiments of the logistic regression models used to produce the figures above, see [here](https://www.comet.ml/kleitoun/logistic-regression-models?shareable=br6L2rrsHv0wCKrImY0IO3PXB) for the model with distance, [here](https://www.comet.ml/kleitoun/logistic-regression-models?shareable=br6L2rrsHv0wCKrImY0IO3PXB) for the model with angle and [here](https://www.comet.ml/kleitoun/logistic-regression-models?shareable=br6L2rrsHv0wCKrImY0IO3PXB) for the model with both distance and angle.

# Question 4
<div style="text-align: justify">
    New features have been added to our dataframe ; they are listed as below:<br>
    <b>-game_seconds:</b> <i>number of seconds elapsed since the beginning of the game, regardless of period</i><br>
    <b>-empty_net:</b> <i> 1 if net empty, 0 otherwise</i><br>
    <b>-gwg:</b> <i> if shot resulted in a goal, specifies if the goal was game winning or not</i><br>
    <b>-distance_from_net:</b> <i>distance of the shot from the net</i><br>
    <b>-angle_from_net:</b> <i>angle at which the shot was taken with respect to the other teams net</i><br>
    <b>-shot_type_#TYPE:</b> <i>one-hot encoded as 1 if from {Backhand, Deflected, Slap shot, Snap shot, Tip-in, Wrap-around, Wrist shot} and 0 otherwise</i><br>
    <b>-previous_event_type:</b> <i>type of the event before the shot happened</i><br>
    <b>-previous_x_(y_)coordinates:</b> <i>x and y-coordinates of the previous event</i><br>
    <b>-previous_event_period:</b> <i>period at which the previous event occured</i><br>
    <b>-previous_attacking_team:</b> <i>team that had possession of the puck during the previous event</i><br>
    <b>-previous_event_game_seconds:</b> <i>number of seconds elasped from the beginning of the game until the previous event happened, regardless of period</i><br>
    <b>-time_since_last_event:</b> <i>difference in game seconds between current event and previous event</i><br>
    <b>-distance_from_last_event:</b> <i>distance between the current event and the previous one</i><br>
    <b>-time_since_powerplay:</b> <i>seconds elapsed since the first powerplay (can cumulate if multiple ones happen)</i><br>
    <b>-rebound:</b> <i>1 if current event and previous event are both shots, 0 otherwise</i><br>
    <b>-overtime:</b> <i>1 if the shot was taken during overtime, 0 otherwise</i><br>
    <b>-speed:</b> <i>ratio of time_since_last_event with respect to distance_from_last_event</i><br>
    <b>-power_play:</b> <i>1 if attacking team has a players advantage, 0 otherwise</i><br>
    <b>-penalty_kill:</b> <i> 1 if attacking team has a players disadvantage, 0 otherwise</i><br>
    <b>-change_in_angle:</b> <i>if the previous was also a shot, compute the difference in the shot angle of the previous and current events</i><br>
    <b>-home_players:</b> <i>number of home players on the rink during that event</i><br>
    <b>-away_players:</b> <i>number of away players on the rink during that event</i><br>
    <b>-5/4/3v5/4/3:</b> <i>description of a players distribution on the rink ; either a power play if left number greater than right and penalty kill othwerise, referencing the attacking team</i><br>
    <b>-goalie_(#GOALIE_NAME):</b> <i>one-hot encoding of goalies ; 1 in the column corresponding to the specific goalie and 0 othwerwise</i><br>
    <b>-attacking_team_(#TEAM_NAME):</b> <i>one-hot encoding of the attacking team ; 1 in the column corresponding to the attacking team and 0 otherwise</i><br>
    </div>
[See experiment here.](https://www.comet.ml/kleitoun/feature-engineering-data?shareable=Ojw17s19mRnbDX1lQD5wptbbK)


# Question 5

## 5.1
<div style="text-align: justify">
    <b>Train/Validation Setup:</b><br>
We choose to 'stratify' over the target class variable, due to the high imbalance in it.
It is always desirable to split the dataset into train and validation sets in a way that preserves the same proportions of datapoints in each class as in the original complete dataset. This is important in order for the model to see fair number of examples from both the classes(0,1 in this case) during training.
Shuffle is used to prevent data from having all similar samples together, which can harm generalization capacity later. Random state is used to ensure reproducibility of the same aplit.

Both of these baseline models (using Logistic Regression and XGBoost) utilize same 2 features: shot distance and shot angle, while XGBoost performs better with an ROC score of ~0.71 vs ~0.69 thats' achieved using Logistic Regression. This boost can be attributed to model's inherent capability of capturing non-linear relationships.
    </div>
[See experiment here](https://www.comet.ml/kleitoun/milestone-2?shareable=yxT9MNApuLpBSDSAia9v8I1iV)

Feature Importance plot using SHAP:
![image](./figures/milestone2/Q5_1_shap_summary_plot.png)

Following are the performance plots:
![image](./figures/milestone2/Q5_1_xgboostROC_Curve.png)
![image](./figures/milestone2/Q5_1_xgboost_Goal_Rate.png)
![image](./figures/milestone2/Q5_1_xgboost_Cum_Goal.png)
![image](./figures/milestone2/Q5_1_xgboost_Calibration_Curve.png)

## 5.2
<div style="text-align: justify">
    <b>Hyperparameter Tuning:</b><br>
Tuning the hyperparameters often has a huge lmpact on optimizing the decision boundaries achieved otherwise. Given the computational limitations, its hard to try out every permutation, hence it is important to **choose** which set of hyperparameters would we want to search through.<br>
    - <b>n_estimators :</b> XGBoost is trained sequentially, where each subsequent tree tries to learn through the errors made by the previous sequence of trees. This parameter is hence to mention how many of such tress would the algorithm need ideally.<br>
    - <b>learning_rate :</b> Learning rate mainly controls the learning speed of the algorithm<br>
    - <b>max_depth :</b> This determines how deep each of the trees(estimator) can go <br>
    - <b>gamma :</b> Gamma controls the complexity and hence aids preventing overfitting<br>
In order to search through the ideal values for the subset of parameters, we used a Randomized Search(RandomSearchCV), that comes up with random combinations of parameters using the various options we input for each of the parameters. The set yielding the best ROC score is used to train the final model. This is said to be faster and better than Grid Search at times. Following are a few examples of such combinations:<br>
     </div>
- Set 1: {'n_estimators': 150, 'learning_rate': 0.3, 'max_depth': 18, 'gamma': 2},
![image](./figures/milestone2/hmtuning_set_1_roc.png)
- Set 2: {'n_estimators': 300, 'learning_rate': 0.1, 'max_depth': 30, 'gamma': 2},
![image](./figures/milestone2/hmtuning_set_2_roc.png)
- Set 3: {'n_estimators': 150, 'learning_rate': 0.3, 'max_depth': 18},
![image](./figures/milestone2/hmtuning_set_3_roc.png)
- Set 4: {'n_estimators': 300, 'learning_rate': 0.1, 'max_depth': 30, 'gamma': 2, 'min_child_weight': 0.25}
![image](./figures/milestone2/hmtuning_set_4_roc.png)
<div style="text-align: justify">
Set 4 yields the best ROC AUC score.(We do not concentate on accuracy due to the huge data imbalance)
Apart from this, a performance boost was find post adding the class weights. As setting these using 'balanced' method did not have any effect, weights were manually set according to the ratio of samples of each class in the train set, to aid a more fair training environment. (This should ideally be the same, but has been a known peculiarity for a while).
A reasonable boost in performace as compared to the Baseline model(~0.71) was achieved, scoring(ROC AUC) ~0.761.
    </div>

[See experiment here](https://www.comet.ml/kleitoun/milestone-2?shareable=yxT9MNApuLpBSDSAia9v8I1iV)

Following are the performance plots:
![image](./figures/milestone2/Q5_2_xgboost_ROC_Curve.png)
![image](./figures/milestone2/Q5_2_xgboost_Goal_Rate.png)
![image](./figures/milestone2/Q5_2_xgboost_Cum_Goal.png)
![image](./figures/milestone2/Q5_2_xgboost_Calibration_Curve.png)

## 5.3
<div style="text-align: justify">
<b>Feature Selection:</b><br>
We use the model trained above to fetch the feature importance scores, or the features that XGBoost found impactful to make the decisions. Following is a plot depicting the same:
    </div>
![image](./figures/milestone2/Q5_feature_importance.png)
<div style="text-align: justify">
We then use Recursive Feature Elimination for feature selection. This is essentially a method where we iteratively set a threshold over the feature importance scores from above, to remove one(or more) features and choose the setting that yields the best score.
The second best alternative uses 28 out of the 31 features extracted before in part 4.
While the best used all of the features(model with only hyperparameter tuning and class weights). The second best used 28 out of 31 features.
    </div>

[See experiment here](https://www.comet.ml/kleitoun/milestone-2?shareable=yxT9MNApuLpBSDSAia9v8I1iV)

Following are the performance plots for the second best model with 28 features:
![image](./figures/milestone2/Q5_3_xgboost_ROC_Curve.png)
![image](./figures/milestone2/Q5_3_xgboost_Goal_Rate.png)
![image](./figures/milestone2/Q5_3_xgboost_Cum_Goal.png)
![image](./figures/milestone2/Q5_3_xgboost_Calibration_Curve.png)

# Question 6

## Random Forest

<div style="text-align: justify">
Our first experiment was with random forests. In order to increase the model's prediction speed and accuracy we removed some irrelevant features from our initial dataset and only kept the relevant ones and the ones added in Question 4. We also decided to remove the one hot encoded features of goalies and attacking teams in hope to reduce bias towards certain teams; this could have been a great way to improve our model but wasn't done due to a lack of processing power. We did a grid search using only the number of estimators and the maximum depth of each tree as our hyperparameters. The GridSearchCV chose a max_depth of 16 and a number of estimators of 50 as the best combination. 
    We also encountered an issue with plotting our figures for our Random Forest model. The computed values for our metrics were different than what appeard on the figures. For example, we obtained an AUC of 0.539 but the plot is showing an AUC of 0.861. We couldn't figure out the issue in time and decided to report our issues with this experiment. This issue does not seem to affect any of the the other experiments.
    </div>
[See here for the experiment of the Random Forest Model.](https://www.comet.ml/kleitoun/random-forest?shareable=JM4DUA7zcrHT3pP7MoGstDeal)
![image](./figures/milestone2/Q6Random Forest_ROC_Curve.png)
![image](./figures/milestone2/Q6Random Forest_Goal_Rate.png)
![image](./figures/milestone2/Q6Random Forest_Cum_Goal.png)
![image](./figures/milestone2/Q6Random Forest_Calibration_Curve.png)


## Neural Network
<div style="text-align: justify">
Our next experiment was with Neural Networks. We decided to go with a Softmax classifier with RELU activation functions and a cross entropy loss built in pytorch. At first, we tried to include as many features as possible (such as attacking team and defending goalie one-hot encoded), but the models would get stuck in a local minimum and predict a goal probability of 0% for every single datapoint. We removed some of these features manually, after which our models where able to train proprely. After some hyperparameter tuning, we settled on a 4-layer network with hidden dimensions of 16, 10 and 8,  trained with Adam and a learning rate of 0.001. We retrained the model for 2000 more epochs since it looked like it may not have finished converging during our grid search. This ended up being the model we chose to test in question 7. The four diagnostic plots are show below:
    </div>
[See here for grid search](https://www.comet.ml/kleitoun/neural-net/53b11ebc6a624822b5f4126253312851) and [here for the final model trained for 2000 more epochs.](https://www.comet.ml/kleitoun/neural-net/f6b8ce4a119e4143abbc98bbbc55edd8)

![image](./figures/milestone2/Q6_neural_net_ROC_Curve.png)
![image](./figures/milestone2/Q6_neural_net_Goal_Rate.png)
![image](./figures/milestone2/Q6_neural_net_Cum_Goal.png)
![image](./figures/milestone2/Q6_neural_net_Calibration_Curve.png)

<b>Considerations related to training/validation split</b>
<div style="text-align: justify">
So far, our experiments had been performed on train and validation sets that were generated randomly from the 2015/16-2018/19 data. However, this is not how our model will be tested: we will be using 2015/16-2018/19 for modeling and 2019/20 for testing. This is often how machine learning models are used in practice, where past data is used to model the future. We felt that this notion of time could have a non-negligible impact on the performance of our model. For example, changes in strategy or in how events are logged can occur over time, which could make a model trained on 2015/16-2018/19 perform poorly for the season of 2019/20. If this was the case, our experiments so far could have been overstimating the performance of our model. To estimate the impact that time could have, we trained a Neural Network with the same architecture as above, this time using 2015/16-2017/18 as our training set and 2018/19 as our validaiton set. The results here were ecouraging: our model performed as well as in our previous experiment, as can be seen below. This increases our confidence that our best model will generalise fairly well to the 2019/20 data.
    </div>
The commet log can be found [here.](https://www.comet.ml/kleitoun/neural-net/d193966f5e7d4a439c0f7153120a84cd)

![image](./figures/milestone2/Q6_neural_net_ROC_Curve_season_split.png)
![image](./figures/milestone2/Q6_neural_net_Goal_Rate_season_split.png)
![image](./figures/milestone2/Q6_neural_net_Cum_Goal_season_split.png)
![image](./figures/milestone2/Q6_neural_net_Calibration_Curve_season_split.png)

## Light GBM

<div style="text-align: justify">
With an objective to come up with the best model for predicting goals (class 1); Light Gradient Boosting Machine (LightGBM), a gradient boosting algorithm is chosen.
On the Train/Validation setup front, every third game is considered to be part of validation set. Hence, 2/3 of total games form the training set and 1/3 are made part of the test set

All the numeric features from Q4, including the ones already one-hot encoded, are chosen to train and validate the data using LightGBM algorithm.

The chosen features are normalized using the min-max scalar before trying other advanced feature selection techniques.

PCA is used to reduce the dimensionality of the problem statement in view of selecting the best attributes that capture the max variance in the dataset. The number of required principal components is set to the number of available features initially and a cumulative sum of the variance of each principal component is plotted against the number of features. A threshold of 95% variance is chosen as a cut-off to pick the best principal components that are sufficiently deterministic for further analysis.

Once the critical principal components are chosen, grid search is performed using GridSearchCV on the parameters to finetune them for an optimal solution. GridSearchCV includes 5 fold cross validation by default. Grid search is performed over 'max_depth', 'n_estimators' & 'min_samples_leaf' to find the best parameters that satisfy the scoring metric of 'AUC'.

Once the best parameters are predicted by GridSearchCV, those parameters are used to train the LightGBM model with a learning rate of 0.1 and check various metrics like AUC, Confusion Matrix, Calibration curve to estimate the overall performance of the model.

LightGBM performs well compared to the baseline model (logistic regression) but is not the best of the lot compared to other classifiers tried. More in depth hyperparameter tuning is indeed required to bring the model to an optimal performance rating.
    </div>

![image](./figures/milestone2/Q6LightGBM_ROC_Curve.png)
![image](./figures/milestone2/Q6LightGBM_Goal_Rate.png)
![image](./figures/milestone2/Q6LightGBM_Cum_Goal.png)
![image](./figures/milestone2/Q6LightGBM_Calibration_Curve.png)
[See here for the experiment of the Light GBM.](https://www.comet.ml/kleitoun/lightgbm-q6?shareable=UgSv6P0oDpM9BNhliaQ1iDwG2)

# Question 7

## Testing on regular season
![image](./figures/milestone2/Q7_R_combined_ROC_Curve.png)
![image](./figures/milestone2/Q7_R_combined_Goal_Rate.png)
![image](./figures/milestone2/Q7_R_combined_Cum_Goal.png)
![image](./figures/milestone2/Q7_R_Calibration_Curve.png)

## Discussion
<div style="text-align: justify">
    Looking at our ROC plot, we can see that the best performing model is the XGBoost. The AUC obtained for XGBoost and the Neural Network are quite similar, which is to be expected since they were the best performing models on the validation set. The third best performing model is the Logistic Regression model trained only on the distance feature. The Logistic Regression model trained on the angle feature only behaves similarly as on the validation set - it is really close to the Random Baseline. As for the combined goal rate figure, we aren't seeing many changes from the plots obtained on the validation set - an indication that our model perform similarly on the training set as on the validation set. The calibration curve also shows that the Neural Network and the XGBoost models are the best performing ones - even though the XGBoost model remains further away from the ideal calibration line while the NN oscilattes over and under it. Overall, the results are expected - a slight deterioration compared to our results on the validation sets.
    </div>

## Testing on playoffs
![image](./figures/milestone2/Q7_P_combined_ROC_Curve.png)
![image](./figures/milestone2/Q7_P_combined_Goal_Rate.png)
![image](./figures/milestone2/Q7_P_combined_Cum_Goal.png)
![image](./figures/milestone2/Q7_P_Calibration_Curve.png)

## Discussion
<div style="text-align: justify">
    The ROC plot shows that for the XGBoost, the Neural Network and the Logistic Regression model trained on the angle feature, the performance drops by a little bit compared to the regular season, although the overall shape is similar. For the Logistic Regression models trained on the distance feature only and on both the distance and angle features the results don't look good. Their ROC is below 0.5, making them worse than a random classifier. Looking at their shape, we see the curves start off looking reasonnable and then cut to below the diagonal near the middle. One potential explanation for this is that the coordinate data could be encoded differently in the playoffs, causing our 'distance' feature to be wrong half of the time (i.e. half the time the coordinates are on the wrong side of the ice), which causes the model to perform very poorly on datapoints whose coordinates are wrong. Finally, if we look at our calibration curves, we can observe great differences for our Neural Network and our best XGBoost model - after the mean predicted value goes past 0.5, the Neural Network almost doesn't predict any positives. The XGBoost, on the other hand, is closer to its regular season counterpart, but predicts less positives as well.
    </div>
